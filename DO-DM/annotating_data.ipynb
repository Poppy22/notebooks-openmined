{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d15e1e-b3fe-4c69-9c22-c9aaf092e268",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8b5d2-8509-4b8d-88df-0f94a075cd45",
   "metadata": {},
   "source": [
    "These instructions are intended for data owners / data managers. This tutorial will cover annonating data and uploading it to the domain node.\n",
    "\n",
    "To be able to continue, you should have received instructions from the IT specialist about how to access the domain node UI (URL, port, credentials, etc). If not, check with the designated IT responsible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d490ab-cd74-4b6c-a252-448968dc360a",
   "metadata": {},
   "source": [
    "## Logging into the domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d6a2e-2791-4e5b-bfaa-0422e0dd1648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3f8a9-726a-4d46-aa08-aa9af5c4ace5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the login credentials should have been provided by the IT specialist\n",
    "\n",
    "try:\n",
    "    domain_client = sy.login(\n",
    "      port=8081,\n",
    "      email=\"info@openmined.org\",\n",
    "      password=\"changethis\"\n",
    "   )\n",
    "except Exception as e:\n",
    "    print(\"Unable to login. Please check your domain is up with `!hagrid check localhost:8081`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5caafd-5652-4ee6-abff-d9e7f3881c75",
   "metadata": {},
   "source": [
    "## Prepare your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edf03fe-30f1-41d4-9b9c-8799b8fbb908",
   "metadata": {},
   "source": [
    "Before uploading the dataset, make sure it contains the necessary data and in the right format.\n",
    "\n",
    "PySyft support Pandas Dataframes objects, so make sure your dataset can be parsed as such using the [Python Pandas](https://pandas.pydata.org) library.\n",
    "\n",
    "For example, we will use a simple dataset consisting of n=4 data subjects, their age and their hourly income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e7426e-7e8e-44e0-8db1-fa05c442865a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  Age  Hourly Income\n",
      "0  011   40             20\n",
      "1  015   39             25\n",
      "2  022    9             32\n",
      "3  034    8             18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'ID': ['011', '015', '022', '034'],\n",
    "     'Age': [40, 39, 9, 8],\n",
    "     'Hourly Income': [20, 25, 32, 18]  }\n",
    "\n",
    "dataset = pd.DataFrame(data)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b35a4-5021-4a0b-9587-0d6b75f52f91",
   "metadata": {},
   "source": [
    "## Annotate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3891847-de06-4f33-bc4e-5828a64233fd",
   "metadata": {},
   "source": [
    "After preparing the dataset in the right format, the next step is to annotate it with **privacy-specific metadata**, which will allows the PySyft library to protect and adjust the visibility different Data Scientists will have into any one of the data subjects.\n",
    "\n",
    "Each feature needs to define the appropriate minimum and maximum ranges (`lower_bound` and `upper_bound`), which represent the theoretical range of values that could be learned about that aspect.\n",
    "\n",
    "**If your project has a training set, validation set and test set, you must annotate each data set with metadata as described.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039aa44f-b5cc-47a7-8dad-905eab46dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the unique identifier for each data subject\n",
    "data_subjects = sy.DataSubjectArray.from_objs(dataset[\"ID\"])\n",
    "\n",
    "# adding metadata for feature 'age'\n",
    "age_data = sy.Tensor(dataset[\"Age\"]).annotate_with_dp_metadata(\n",
    "   lower_bound=0, upper_bound=120, data_subject=data_subjects\n",
    ")\n",
    "\n",
    "# adding metadata for feature 'data'\n",
    "hourly_income_data = sy.Tensor(dataset[\"Hourly Income\"]).annotate_with_dp_metadata(\n",
    "   lower_bound=10, upper_bound=500, data_subject=data_subjects\n",
    ")\n",
    "\n",
    "# ...this needs to be done for every feature, and for every dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2817e-f281-469e-bb01-61d431f4c41c",
   "metadata": {},
   "source": [
    "## Upload the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee281e-6dde-4b96-b331-2fc6685d85ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once it's annotated, the dataset can be uploaded to the domain server to be used by data scientists.\n",
    "\n",
    "Add details like `name` and `description` so that data scientists can more easily come across your dataset.\n",
    "\n",
    "You uploaded dataset is formed of the tensors with annotated data; it is not the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb172422-8a84-4ea2-9f52-bb0fe81b7854",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_client.load_dataset(\n",
    "   name=\"Age_Income_Dataset\",\n",
    "   assets={\n",
    "      \"Age_Data\": age_data, # annotated age data\n",
    "      \"Hourly_Income\": hourly_income_data # annotated income data\n",
    "   },\n",
    "   description=\"Our dataset contains...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f12a2e-7e7b-42e0-9f33-8860c2cdb9e7",
   "metadata": {},
   "source": [
    "If your project has multiple datasets, you can upload them separately using the `load_dataset` function above. For example, if your dataset has `testing`, `training` and `validation` datasets, all of them can be uploaded separately, with the corresponding assets, names and descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af3366-e64f-44be-bf54-e1181caec87e",
   "metadata": {},
   "source": [
    "## Closing thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a914c-19a2-43dd-b598-c7e46822f38c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Congrats on uploading the dataset! Here's what you should have achieved from this tutorial:\\\n",
    "✅ transformed the dataset to have the right format, if needed\\\n",
    "✅ annotated the dataset\\\n",
    "✅ uploaded the dataset to the domain node"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
